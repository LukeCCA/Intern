{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pandas as pd \n",
    "import ast\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from ckiptagger import WS\n",
    "from collections import Counter\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"IVR\"\n",
    "WS_SET = [\"jieba\", \"ckip\", \"mix_0\", \"mix_1\"]\n",
    "WORD_SEGMENTER = WS_SET[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa21925fc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa21925fc88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa21bd32470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa21bd32470>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa21bb684a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa21bb684a8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa2176bcf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa2176bcf98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa28fe9f940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa28fe9f940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "if WORD_SEGMENTER == \"ckip\" or WORD_SEGMENTER.find(\"mix\") != -1:\n",
    "    ws = WS(\"../ckiptagger/data\")\n",
    "\n",
    "def get_model_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "#     df = df.sort_values(by=[\"labels\"], ignore_index=True)\n",
    "    src_labels = sorted(set(df.labels.tolist()))\n",
    "    df[\"labels\"] = [src_labels.index(l) for l in df.labels.tolist()]\n",
    "    texts = df[\"texts\"]\n",
    "    labels = df[\"labels\"]\n",
    "    num_labels = list(Counter(labels).values())\n",
    "    \n",
    "    data = {\"corpus\":[], \"labels\":[], \"src_texts\":[], \"src_label\":[]}\n",
    "    \n",
    "    for i, t in (enumerate(texts)):\n",
    "        label = labels[i]\n",
    "\n",
    "        if WORD_SEGMENTER == \"ckip\":\n",
    "            sentence_seg = ws([t])[0]\n",
    "        elif WORD_SEGMENTER == \"jieba\":\n",
    "            sentence_seg = jieba.lcut(t)\n",
    "        elif WORD_SEGMENTER == \"mix_0\":\n",
    "            temp = ws([t])[0]\n",
    "            sentence_seg = []\n",
    "            for seg_t in temp:\n",
    "                sentence_seg += jieba.lcut(seg_t)\n",
    "        elif WORD_SEGMENTER == \"mix_1\":\n",
    "            temp = jieba.lcut(t)\n",
    "            sentence_seg = []\n",
    "            for seg_t in temp:\n",
    "                sentence_seg += ws([seg_t])[0]\n",
    "    \n",
    "        sentence_seg = [seg_t for seg_t in sentence_seg if seg_t!=' ']\n",
    "#         seg_texts = ' '.join(sentence_seg)\n",
    "        \n",
    "        data[\"corpus\"] += [sentence_seg]\n",
    "        data[\"labels\"] += [label]\n",
    "        data[\"src_texts\"] += [t]\n",
    "        data[\"src_label\"] += [src_labels[label]]\n",
    "    return data, num_labels # Dict[List], List[Int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(corpus, data):\n",
    "    with open(\"extend_dataset/IVR/tags.pkl\", 'rb') as fp:\n",
    "        tags = pickle.load(fp)\n",
    "\n",
    "    tf_list = []\n",
    "    for texts in corpus:\n",
    "        tf = [0] * len(tags)\n",
    "        for text in texts:\n",
    "            i = 0\n",
    "            for key, value in tags.items():\n",
    "                value = sorted(value, key=len, reverse=True)\n",
    "                value = [v for v in value if len(v)==len(text)]\n",
    "                if value == []:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                big_regex = re.compile('|'.join(map(re.escape, value)))\n",
    "                temp = big_regex.findall(text)\n",
    "                tf[i] += len(temp)\n",
    "                i += 1\n",
    "        tf_list += [tf]\n",
    "    data[\"tf\"] = tf_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, num_labels = get_model_data(f\"data/{DATASET}/train.tsv\")\n",
    "data_test, _ = get_model_data(f\"data/{DATASET}/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = get_tf(data_train[\"corpus\"], data_train)\n",
    "data_test = get_tf(data_test[\"corpus\"], data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extend_dataset/IVR/tags.pkl\", 'rb') as fp:\n",
    "    feature_names = pickle.load(fp)\n",
    "feature_names = list(feature_names.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seg\n",
    "# vec = np.stack(data_train[\"tf\"])\n",
    "\n",
    "# all\n",
    "# data_train = pd.read_csv(\"data/IVR/train.tsv\", sep='\\t')\n",
    "# vec = np.stack(data_train[\"tf\"].apply(ast.literal_eval).tolist())\n",
    "# data_train[\"src_label\"] = data_train[\"labels\"]\n",
    "# data_test = pd.read_csv(\"data/IVR/test.tsv\", sep='\\t')\n",
    "\n",
    "# rule\n",
    "data_train = pd.read_csv(\"data/IVR/class_label.tsv\", sep='\\t')\n",
    "vec = np.stack(data_train[\"vectors\"].apply(ast.literal_eval).tolist())\n",
    "data_train[\"src_label\"] = data_train[\"labels\"]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(vec, data_train[\"labels\"])\n",
    "\n",
    "# rule uni\n",
    "# data_train = pd.read_csv(\"data/IVR/class_label.tsv\", sep='\\t')\n",
    "# data_train[\"vectors\"] = data_train[\"vectors\"].apply(ast.literal_eval).tolist()\n",
    "# data_train[\"src_label\"] = data_train[\"labels\"]\n",
    "\n",
    "label_set = sorted(set(data_train[\"src_label\"]))\n",
    "\n",
    "# rule uni\n",
    "# vec = []\n",
    "# for l in label_set:\n",
    "#     df = data_train[data_train[\"labels\"] == l]\n",
    "# #     print(l, df[\"vectors\"].shape[0])\n",
    "#     if df[\"vectors\"].shape[0] == 0:\n",
    "#         continue\n",
    "#     elif df[\"vectors\"].shape[0] == 1:\n",
    "#         vec += df[\"vectors\"].tolist()\n",
    "#     else:\n",
    "#         temp = np.stack(df[\"vectors\"])\n",
    "#         vec += [np.logical_or.reduce(temp)]\n",
    "# vec = np.stack(vec)\n",
    "# print(vec.shape)\n",
    "\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(vec, label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree_all.pdf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=feature_names,  \n",
    "                     class_names=label_set,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(filename='tree_all')\n",
    "# graph\n",
    "# tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(clf, test_data, vectors=None, show_err=False):\n",
    "#     test_data = test_data[test_data[\"labels\"] != \"133_2\"]\n",
    "#     test_data = test_data[test_data[\"labels\"] != \"31_2\"]\n",
    "#     test_data = test_data[test_data[\"labels\"] != \"31_3\"]\n",
    "#     test_data = test_data[test_data[\"labels\"] != \"31_4\"]\n",
    "#     test_data = test_data[test_data[\"labels\"] != \"31_5\"]\n",
    "\n",
    "    if vectors is None:\n",
    "        vectors = test_data.tf.apply(ast.literal_eval).tolist()\n",
    "        vectors = np.stack(vectors)\n",
    "    pred = clf.predict(vectors)\n",
    "    \n",
    "    flag = 0\n",
    "    if isinstance(pred[0], np.int64):\n",
    "        print(\"type convert\")\n",
    "        flag = 1\n",
    "        pred = np.array([label_set[l] for l in pred])\n",
    "    \n",
    "    ans = test_data.labels.tolist()\n",
    "    ans = np.array(ans)\n",
    "    \n",
    "#     print(type(pred[0]), ans)\n",
    "    \n",
    "    correct = np.count_nonzero(pred==ans)\n",
    "    acc = correct / len(test_data[\"labels\"])\n",
    "    print(acc)\n",
    "\n",
    "    \n",
    "    if show_err:\n",
    "        c = [0 for _ in range(63)]\n",
    "        print(\"| ans | pred | texts | keyword |  |\")\n",
    "        print(\"|-|-|-|-|-|\")\n",
    "        if flag: # seg\n",
    "            for idx in np.nonzero(pred!=ans)[0]:\n",
    "                print(f\"| {ans[idx]} | {pred[idx]} | {data_test['corpus'][idx]} | {[feature_names[i] for i in np.nonzero(data_test['tf'][idx])[0] ]} |  |\")\n",
    "        else:\n",
    "            for idx in np.nonzero(pred!=ans)[0]:\n",
    "                c[label_set.index(ans[idx])] += 1\n",
    "                print(f\"| {ans[idx]} | {pred[idx]} | {data_test['texts'][idx]} | { [feature_names[i] for i in np.nonzero(vectors[idx])[0] ] } |  |\")\n",
    "#                 print(vectors[idx])\n",
    "        print(c, sum(c))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(data_test[\"labels\"])\n",
    "調 改 辦 借 日 砍 刪 欠 嗎 轉 找 補 錢 爆 密 繳 交 扣 剩 還 寄 傳 補 花 刷 取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44343065693430656\n",
      "| ans | pred | texts | keyword |  |\n",
      "|-|-|-|-|-|\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e057a50a372b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-d7f8263d0c7a>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(clf, test_data, vectors, show_err)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"| {ans[idx]} | {pred[idx]} | {data_test['texts'][idx]} | { [feature_names[i] for i in np.nonzero(vectors[idx])[0] ] } |  |\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;31m#                 print(vectors[idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_test' is not defined"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"data/IVR/test.tsv\", sep='\\t')\n",
    "# seg\n",
    "# vec = np.stack(data_test[\"tf\"])\n",
    "# get_prediction(clf, test_data, vec, show_err=True)\n",
    "\n",
    "# all\n",
    "get_prediction(clf, test_data, show_err=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

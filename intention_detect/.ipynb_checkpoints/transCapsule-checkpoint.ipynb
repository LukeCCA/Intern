{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import jieba\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import opencc\n",
    "from ckiptagger import WS\n",
    "from datetime import datetime,timezone,timedelta\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"SMP2018\"\n",
    "NUM_ROUTING_ITERATIONS = 4\n",
    "KERNEL_SIZE = 2\n",
    "HIDDEN_SIZE = 300\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 64\n",
    "MODEL_PATH = \"model/transCapsule\" # svae/load model name/path\n",
    "EPOCHS = 10\n",
    "MAX_LENGTH = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(msg=\"\"):\n",
    "    dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "    print(str(dt2)[:-13] + '\\t' + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-level 顯示此模型裡的 modules\n",
    "def model_info(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"total params: {total_params}\")\n",
    "#     print(model.device)\n",
    "    print(\"\"\"\n",
    "    name            module\n",
    "    ----------------------\"\"\")\n",
    "    for name, module in model.named_children():\n",
    "        if name == \"bert\" or name==\"0\":\n",
    "            for n, _ in module.named_children():\n",
    "                print(f\"{name}:{n}\")\n",
    "    #             print(_)\n",
    "        else:\n",
    "            print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "timestamp()\n",
    "W2V = KeyedVectors.load_word2vec_format(\"../zhwiki_20180420_300d.txt.bz2\")\n",
    "timestamp()\n",
    "W2V.save('w2v_300d.kv')\n",
    "timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "W2V = KeyedVectors.load('w2v_100d.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ckipDict(dict):\n",
    "    def __init__(self, vocab, vec):\n",
    "        self.vocab = vocab\n",
    "        self.vec = vec\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if key in self.vocab:\n",
    "            idx = self.vocab.index(key)\n",
    "            return self.vec[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = np.load(\"../ckiptagger/data/embedding_word/token_list.npy\").tolist()\n",
    "vec = np.load(\"../ckiptagger/data/embedding_word/vector_list.npy\")\n",
    "W2V = ckipDict(voc, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f6382ac1278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f6382ac1278>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f6364e90518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f6364e90518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f64581fbe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f64581fbe48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f6382adeb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f6382adeb38>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6382acd550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6382acd550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "ws = WS(\"../ckiptagger/data\")\n",
    "def get_model_data(file_path, hidden_size=HIDDEN_SIZE):\n",
    "    t2s_converter = opencc.OpenCC(\"t2s.json\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    src_labels = sorted(set(df.labels.tolist()))\n",
    "    num_labels = len(src_labels)\n",
    "    df[\"labels\"] = [src_labels.index(l) for l in df.labels.tolist()]\n",
    "    data = []\n",
    "    texts = df[\"texts\"]\n",
    "\n",
    "    labels = df[\"labels\"]\n",
    "    oov = []\n",
    "    for i, t in (enumerate(texts)):\n",
    "        seg_texts = []\n",
    "        label = labels[i]\n",
    "        sentence_seg = [seg_t for seg_t in ws([t])[0] if seg_t!=' '] # ckip segment\n",
    "        \n",
    "        data_dict = dict()\n",
    "        emb = []\n",
    "\n",
    "        for seg_t in sentence_seg:\n",
    "            if seg_t in W2V.vocab:\n",
    "                emb += [W2V[seg_t].tolist()]\n",
    "                seg_texts += [seg_t]\n",
    "                \n",
    "            # if zh_tw not in w2v vocab try use zh_cn\n",
    "            elif t2s_converter.convert(seg_t) in W2V.vocab:\n",
    "                emb += [W2V[t2s_converter.convert(seg_t)].tolist()]\n",
    "                seg_texts += [seg_t]\n",
    "                \n",
    "            # also not in w2v vocab, try jeiba \n",
    "            else:\n",
    "                for sseg_t in jieba.cut(seg_t):\n",
    "                    if sseg_t == ' ':\n",
    "                        continue\n",
    "                    seg_texts += [sseg_t]\n",
    "\n",
    "                    if sseg_t in W2V.vocab:\n",
    "                        emb += [W2V[sseg_t]]\n",
    "\n",
    "                    else: # oov: mean vector of each character\n",
    "#                         print(f\"{sentence_seg} {seg_t} {sseg_t}\")\n",
    "                        temp = []\n",
    "                        for char in sseg_t:\n",
    "                            if char in W2V.vocab:\n",
    "                                temp += [W2V[char]]\n",
    "                        if len(temp) != 0:\n",
    "                            emb += [np.stack(temp).mean(axis=0).tolist()]\n",
    "                        oov += [sseg_t]\n",
    "        \n",
    "#         if len(emb) < MAX_LENGTH: # padding\n",
    "#             emb += [[0]*hidden_size] * ((MAX_LENGTH)-len(emb))\n",
    "        \n",
    "        data_dict = {\"emb\": emb, \"label\":label, \n",
    "                     \"src_texts\": t, \"src_label\": src_labels[label],\n",
    "                     \"seg_texts\": seg_texts}\n",
    "        data += [data_dict]\n",
    "    \n",
    "    print(f\"oov: {len(oov)} {len(set(oov))}, {sorted(set(oov))}\")\n",
    "    \n",
    "    return data, num_labels # List[Dict[List]] = List[tokenizer output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, num_labels = get_model_data(f\"data/{DATASET}/train.tsv\")\n",
    "data_test, _ = get_model_data(f\"data/{DATASET}/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_train, f\"bert_data/{DATASET}/transCapsule_train.pt\")\n",
    "torch.save(data_test, f\"bert_data/{DATASET}/transCapsule_test.pt\")\n",
    "\n",
    "del ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 41 if DATASET == \"base\" else 31\n",
    "data_train = torch.load(f\"bert_data/{DATASET}/transCapsule_train.pt\")\n",
    "data_test = torch.load(f\"bert_data/{DATASET}/transCapsule_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 1 5.193997390169639\n"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "for item in data_train:\n",
    "    ls += [np.array(item[\"emb\"]).shape[0]]\n",
    "ls = np.array(ls)\n",
    "print(ls.max(), ls.min(), ls.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# s = \"會計處公用槽j槽檔案刪除時出現您可以向 「oant」\\he開機帳密cd開機帳密m 取得權限來變更此檔案\"\n",
    "# s = \"外匯存款掉異常record not existed\"\n",
    "# s = \"客戶使用不了e指開戶\"\n",
    "# s = \"信卡網銀申請不了\"\n",
    "# s = \"法金企網銀登入不了\"\n",
    "# s = \"企網銀新增權限\"\n",
    "# s = \"需求速報單上面要寫那些東西 j槽刪東西\"\n",
    "# s = \"影像出來的案件編號不同\"\n",
    "# s = \"公文能不能退回或做處理權移轉\"\n",
    "# s = \"ims-3099\"\n",
    "# s = \"過程好像跑出尚未結清\"\n",
    "# s = \"自行轉帳掉異常件4704 帳號幣別已存在\"\n",
    "# s = \"錯誤 '339' :元件 'tabctl32.ocx'或他的\"\n",
    "# s = \"新增行員登入權限\"\n",
    "s = \"點開議價系統出現執行階段錯誤 '339' :元件 'tabctl32.ocx'或它的依存檔並未正確註冊:某檔案遺漏或不正確\"\n",
    "\n",
    "seg = ws([s])[0]\n",
    "print(seg)\n",
    "f = []\n",
    "for t in seg:\n",
    "    f += jieba.lcut(t)\n",
    "print(f)\n",
    "seg = jieba.lcut(s)\n",
    "print(seg)\n",
    "f = []\n",
    "for t in seg:\n",
    "    f += ws([t])[0]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_Dataset(Dataset):\n",
    "    def __init__(self, mode, list_of_bert):\n",
    "        assert mode in [\"train\", \"test\", \"dev\"]\n",
    "        self.mode = mode\n",
    "        self.data = list_of_bert\n",
    "    def __getitem__(self, idx):\n",
    "        emb = torch.tensor(self.data[idx][\"emb\"])\n",
    "        label = torch.tensor(self.data[idx][\"label\"])\n",
    "        return emb, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(sample):\n",
    "    sample.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "    embs, labels = zip(*sample)\n",
    "    max_length = MAX_LENGTH\n",
    "    masks = []\n",
    "    pad_embs = []\n",
    "    \n",
    "    \n",
    "    for e in embs:\n",
    "        # padding\n",
    "        pad_len = max_length - e.shape[0]\n",
    "        padding = torch.zeros(pad_len, HIDDEN_SIZE)\n",
    "        pad_embs += [torch.cat((e, padding)).tolist()]\n",
    "        \n",
    "        # attn masling\n",
    "        masking = [False] * e.shape[0] + [True] * pad_len\n",
    "        masks += [masking]\n",
    "        \n",
    "    pad_embs = torch.tensor(pad_embs)\n",
    "    masks = torch.tensor(masks)\n",
    "    labels = torch.tensor(labels)\n",
    "    return pad_embs, masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorce form: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=MAX_LENGTH):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source from: https://github.com/leftthomas/CapsNet/blob/master/capsule.py\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels, kernel_size=None, stride=None,\n",
    "                 num_iterations=NUM_ROUTING_ITERATIONS):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "\n",
    "        self.num_route_nodes = num_route_nodes\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        if num_route_nodes != -1: # digit_capsules\n",
    "            self.route_weights = nn.Parameter(torch.randn(num_capsules, num_route_nodes, in_channels, out_channels))\n",
    "        else: # primary_capsules\n",
    "            self.capsules = nn.ModuleList(\n",
    "                [nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0) for _ in\n",
    "                 range(num_capsules)])\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_route_nodes != -1: # digit_capsules\n",
    "            priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :] # random initial hat u_j|i\n",
    "            logits = Variable(torch.zeros(*priors.size())) # b_ij = 0\n",
    "            if torch.cuda.is_available():\n",
    "                logits = logits.cuda()\n",
    "            for i in range(self.num_iterations):\n",
    "                probs = F.softmax(logits, dim=2) # c_ij = softmax(b_ij)\n",
    "                outputs = self.squash((probs * priors).sum(dim=2, keepdim=True)) # squash(sum c_ij * hat u_j|i)\n",
    "\n",
    "                if i != self.num_iterations - 1:\n",
    "                    delta_logits = (priors * outputs).sum(dim=-1, keepdim=True) # hat u_j|i * v_j\n",
    "                    logits = logits + delta_logits #b_ij = b_ij + hat u_j|i * v_j\n",
    "        else: # primary_capsules\n",
    "            outputs = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "            outputs = torch.cat(outputs, dim=-1)\n",
    "            outputs = self.squash(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_classifier(nn.Module):\n",
    "    def __init__(self, kernel_size, num_labels, stride=1, hidden_size=HIDDEN_SIZE):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model=hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=12, dim_feedforward=300)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, 1)\n",
    "        self.primary_capsules = CapsuleLayer(num_capsules=1, num_route_nodes=-1,\n",
    "                                in_channels=hidden_size, out_channels=100,\n",
    "                                kernel_size=kernel_size, stride=stride)\n",
    "        N = int((MAX_LENGTH - kernel_size + 1) / stride)\n",
    "        self.intent_capsules = CapsuleLayer(num_capsules=num_labels, num_route_nodes=100 * N,\n",
    "                                            in_channels=1, out_channels=15)\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(15 * num_labels, num_labels),\n",
    "                        nn.Softmax(dim=1))\n",
    "    \n",
    "    def forward(self, word_emb, mask):\n",
    "        word_emb = self.pos_encoder(word_emb)\n",
    "#         mask = self.generate_square_subsequent_mask(word_emb.shape[0]).to(device)\n",
    "        word_emb = self.transformer(word_emb, src_key_padding_mask=mask)\n",
    "        # transpose for conv1d\n",
    "        # (len, batch, hidden_size) -> (batch, channel, len)\n",
    "        word_emb = word_emb.transpose(0,1)\n",
    "        word_emb = word_emb.transpose(1,2)\n",
    "        \n",
    "        word_emb = self.primary_capsules(word_emb)\n",
    "        word_emb = self.intent_capsules(word_emb).squeeze().transpose(0, 1)\n",
    "        word_emb = word_emb.flatten(start_dim=1)\n",
    "        intent_class = self.decoder(word_emb)\n",
    "        \n",
    "        return intent_class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "self.primary_capsules = CapsuleLayer(num_capsules=1, num_route_nodes=-1, in_channels=300, out_channels=100,\n",
    "                                             kernel_size=, stride=)\n",
    "self.digit_capsules = CapsuleLayer(num_capsules=config.NUM_CLASSES, num_route_nodes=100 *  * , in_channels=1,\n",
    "                                           out_channels=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: 3547046\n",
      "\n",
      "    name            module\n",
      "    ----------------------\n",
      "pos_encoder     PositionalEncoding(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "transformer     TransformerEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=300, out_features=300, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
      "      (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "primary_capsules CapsuleLayer(\n",
      "  (capsules): ModuleList(\n",
      "    (0): Conv1d(300, 100, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      ")\n",
      "intent_capsules CapsuleLayer()\n",
      "decoder         Sequential(\n",
      "  (0): Linear(in_features=465, out_features=31, bias=True)\n",
      "  (1): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = intent_classifier(KERNEL_SIZE, num_labels)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            word_embs, masks, labels = [t.to(device) for t in data if torch.is_tensor(t)]\n",
    "            \n",
    "            intent_cls = model(word_embs.transpose(0,1), masks)\n",
    "            \n",
    "            _, pred = torch.max(intent_cls, 1) # _: logits最大數值; pred: 最大數值的 index\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "\n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = intent_Dataset(\"train\", data_train)\n",
    "trainLoader = DataLoader(trainSet, batch_size=TRAIN_BATCH_SIZE, collate_fn=minibatch, shuffle=True)\n",
    "testSet = intent_Dataset(\"test\", data_test)\n",
    "testLoader = DataLoader(testSet, batch_size=TEST_BATCH_SIZE, collate_fn=minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorboard logger'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tensorboard logger\"\"\"\n",
    "# writer = SummaryWriter(f\"runs/{DATASET}/transCapsule/K_{KERNEL_SIZE}_E_{EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 10:50:06\tstart training model/transCapsule from epoch 1 to 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:46<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 10:50:53\t[epoch 1] loss: 247.264\n",
      "[epoch 1] training acc: 0.014789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] testing acc: 0.014286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:47<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 10:52:21\t[epoch 2] loss: 247.260\n",
      "[epoch 2] training acc: 0.014789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] testing acc: 0.014286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:47<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 10:53:49\t[epoch 3] loss: 247.256\n",
      "[epoch 3] training acc: 0.030883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] testing acc: 0.031169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:47<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 10:55:18\t[epoch 4] loss: 247.250\n",
      "[epoch 4] training acc: 0.030883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] testing acc: 0.031169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:49<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-20 10:56:49\t[epoch 5] loss: 247.244\n",
      "[epoch 5] training acc: 0.030883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] testing acc: 0.031169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0ec2f4d17476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 紀錄當前 batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#     torch.save(model.state_dict(), F\"{MODEL_PATH}_E_{str(epoch+1)}.pt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_from = 0\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "timestamp(f\"start training {MODEL_PATH} from epoch {train_from+1} to {EPOCHS}\")\n",
    "for epoch in range(train_from, EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(trainLoader):\n",
    "        word_embs, masks, labels = [t.to(device) for t in data]\n",
    "#         print(word_emb.shape)\n",
    "#         break\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(word_embs.transpose(0,1), masks) # transpose for transformer (len, batch, hidden_size)\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "#     torch.save(model.state_dict(), F\"{MODEL_PATH}_E_{str(epoch+1)}.pt\")\n",
    "    timestamp(f\"[epoch {epoch+1}] loss: {running_loss:.3f}\")\n",
    "#     writer.add_scalar('Loss/cls', running_loss, epoch)\n",
    "\n",
    "    _, acc = get_predictions(model, trainLoader, compute_acc=True)\n",
    "    print(f\"[epoch {epoch+1}] training acc: {acc:.6f}\")\n",
    "#     writer.add_scalar('Acc/train', acc, epoch)\n",
    "\n",
    "    _, acc = get_predictions(model, testLoader, compute_acc=True)\n",
    "    print(f\"[epoch {epoch+1}] testing acc: {acc:.6f}\")\n",
    "#     writer.add_scalar('Acc/test', acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=128)\n",
    "src = torch.rand(10, 32, 512)\n",
    "encoder_layer(src).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 40, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoder = PositionalEncoding(d_model=100)\n",
    "src = torch.rand(32,40,100)\n",
    "pos_encoder(src).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(15,20)\n",
    "src.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

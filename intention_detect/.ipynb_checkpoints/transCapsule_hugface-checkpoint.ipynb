{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import jieba\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import opencc\n",
    "from ckiptagger import WS\n",
    "from datetime import datetime,timezone,timedelta\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import math\n",
    "from transformers import BertConfig, BertModel, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"ATIS\"\n",
    "PRETRAIN = \"embedding_word\"\n",
    "# PRETRAIN = \"embedding_character\"\n",
    "NUM_ROUTING_ITERATIONS = 4\n",
    "KERNEL_SIZE = 2\n",
    "HIDDEN_SIZE = 300\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 64\n",
    "MODEL_PATH = \"model/transCapsule\" # svae/load model name/path\n",
    "EPOCHS = 20\n",
    "MAX_LENGTH = 64\n",
    "LR = 3e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(msg=\"\"):\n",
    "    dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "    print(str(dt2)[:-13] + '\\t' + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-level 顯示此模型裡的 modules\n",
    "def model_info(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"total params: {total_params}\")\n",
    "#     print(model.device)\n",
    "    print(\"\"\"\n",
    "    name            module\n",
    "    ----------------------\"\"\")\n",
    "    for name, module in model.named_children():\n",
    "        if name == \"bert\" or name==\"0\":\n",
    "            for n, _ in module.named_children():\n",
    "                print(f\"{name}:{n}\")\n",
    "    #             print(_)\n",
    "        else:\n",
    "            print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "timestamp()\n",
    "W2V = KeyedVectors.load_word2vec_format(\"../zhwiki_20180420_300d.txt.bz2\")\n",
    "timestamp()\n",
    "W2V.save('w2v_300d.kv')\n",
    "timestamp()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "timestamp()\n",
    "# Convert\n",
    "input_file = '../glove.6B.300d.txt'\n",
    "output_file = '../gensim_glove.6B.300d.txt'\n",
    "glove2word2vec(input_file, output_file)\n",
    "timestamp()\n",
    "# Test Glove model\n",
    "W2V = KeyedVectors.load_word2vec_format(output_file, binary=False)\n",
    "timestamp()\n",
    "W2V.save(\"glove_en_300d.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# W2V = KeyedVectors.load('../LM/w2v_100d.kv')\n",
    "W2V = KeyedVectors.load('../LM/glove_en_300d.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ckipDict(dict):\n",
    "    def __init__(self, vocab, vec):\n",
    "        self.vocab = vocab\n",
    "        self.vec = vec\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if key in self.vocab:\n",
    "            idx = self.vocab.index(key)\n",
    "            return self.vec[idx]\n",
    "        else:\n",
    "            raise BaseException(f\"key error: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = np.load(f\"../ckiptagger/data/{PRETRAIN}/token_list.npy\").tolist()\n",
    "vec = np.load(f\"../ckiptagger/data/{PRETRAIN}/vector_list.npy\")\n",
    "W2V = ckipDict(voc, vec)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# word base\n",
    "ws = WS(\"../ckiptagger/data\")\n",
    "def get_model_data(file_path, hidden_size=HIDDEN_SIZE):\n",
    "    t2s_converter = opencc.OpenCC(\"t2s.json\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    src_labels = sorted(set(df.labels.tolist()))\n",
    "    num_labels = len(src_labels)\n",
    "    df[\"labels\"] = [src_labels.index(l) for l in df.labels.tolist()]\n",
    "    data = []\n",
    "    texts = df[\"texts\"]\n",
    "\n",
    "    labels = df[\"labels\"]\n",
    "    oov = []\n",
    "    for i, t in (enumerate(texts)):\n",
    "        seg_texts = []\n",
    "        label = labels[i]\n",
    "        sentence_seg = [seg_t for seg_t in ws([t])[0] if seg_t!=' '] # ckip segment\n",
    "        \n",
    "        data_dict = dict()\n",
    "        emb = []\n",
    "\n",
    "        for seg_t in sentence_seg:\n",
    "            if seg_t in W2V.vocab:\n",
    "                emb += [W2V[seg_t].tolist()]\n",
    "                seg_texts += [seg_t]\n",
    "                \n",
    "            # if zh_tw not in w2v vocab try use zh_cn\n",
    "            elif t2s_converter.convert(seg_t) in W2V.vocab:\n",
    "                emb += [W2V[t2s_converter.convert(seg_t)].tolist()]\n",
    "                seg_texts += [seg_t]\n",
    "                \n",
    "            # also not in w2v vocab, try jeiba \n",
    "            else:\n",
    "                for sseg_t in jieba.cut(seg_t):\n",
    "                    if sseg_t == ' ':\n",
    "                        continue\n",
    "                    seg_texts += [sseg_t]\n",
    "\n",
    "                    if sseg_t in W2V.vocab:\n",
    "                        emb += [W2V[sseg_t]]\n",
    "\n",
    "                    else: # oov: mean vector of each character\n",
    "#                         print(f\"{sentence_seg} {seg_t} {sseg_t}\")\n",
    "                        temp = []\n",
    "                        for char in sseg_t:\n",
    "                            if char in W2V.vocab:\n",
    "                                temp += [W2V[char]]\n",
    "                        if len(temp) != 0:\n",
    "                            emb += [np.stack(temp).mean(axis=0).tolist()]\n",
    "                        oov += [sseg_t]\n",
    "        \n",
    "#         if len(emb) < MAX_LENGTH: # padding\n",
    "#             emb += [[0]*hidden_size] * ((MAX_LENGTH)-len(emb))\n",
    "        \n",
    "        data_dict = {\"emb\": emb, \"label\":label, \n",
    "                     \"src_texts\": t, \"src_label\": src_labels[label],\n",
    "                     \"seg_texts\": seg_texts}\n",
    "        data += [data_dict]\n",
    "    \n",
    "    print(f\"oov: {len(oov)} {len(set(oov))}, {sorted(set(oov))}\")\n",
    "    \n",
    "    return data, num_labels # List[Dict[List]] = List[tokenizer output]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# character base\n",
    "def get_model_data(file_path, hidden_size=HIDDEN_SIZE):\n",
    "    t2s_converter = opencc.OpenCC(\"t2s.json\")\n",
    "    s2t_converter = opencc.OpenCC(\"s2t.json\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    src_labels = sorted(set(df.labels.tolist()))\n",
    "    num_labels = len(src_labels)\n",
    "    df[\"labels\"] = [src_labels.index(l) for l in df.labels.tolist()]\n",
    "    data = []\n",
    "    texts = df[\"texts\"]\n",
    "    labels = df[\"labels\"]\n",
    "    oov = []\n",
    "    for i, text in (enumerate(texts)):\n",
    "        seg_texts = []\n",
    "        label = labels[i]\n",
    "        sentence_seg = [seg_t for seg_t in jieba.lcut(text) if seg_t!=' '] # ckip segment\n",
    "        \n",
    "        data_dict = dict()\n",
    "        emb = []\n",
    "\n",
    "        for seg_t in sentence_seg:\n",
    "            temp = []\n",
    "            for char in seg_t:\n",
    "                char = char.replace(' ', '').replace('\\t', ''). \\\n",
    "                        replace('？', '?').replace('！', '!'). \\\n",
    "                        replace('～', '~').replace('，', ',')\n",
    "                if char == '':\n",
    "                    continue\n",
    "                    \n",
    "                if char in W2V.vocab:\n",
    "                    if char > u'\\u4e00' and char < u'\\u9fff': # zh_char\n",
    "                        emb += [W2V[char]]\n",
    "                    else: # en_word\n",
    "                        temp += [W2V[char]]                \n",
    "                else:\n",
    "                    char = s2t_converter.convert(char)\n",
    "                    if char in W2V.vocab:\n",
    "                        emb += [W2V[char]]\n",
    "                    else:\n",
    "                        oov += [char]\n",
    "            if temp != []:\n",
    "                emb += [np.stack(temp).mean(axis=0).tolist()]\n",
    "\n",
    "        data_dict = {\"emb\": emb, \"label\":label, \n",
    "                     \"src_texts\": text, \"src_label\": src_labels[label],\n",
    "                     \"seg_texts\": seg_texts}\n",
    "        data += [data_dict]\n",
    "    \n",
    "    print(f\"oov: {len(oov)} {len(set(oov))}, {sorted(set(oov))}\")\n",
    "    \n",
    "    return data, num_labels # List[Dict[List]] = List[tokenizer output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en\n",
    "def get_model_data(file_path, hidden_size=HIDDEN_SIZE):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    src_labels = sorted(set(df.labels.tolist()))\n",
    "    num_labels = len(src_labels)\n",
    "    df[\"labels\"] = [src_labels.index(l) for l in df.labels.tolist()]\n",
    "    data = []\n",
    "    texts = df[\"texts\"]\n",
    "    labels = df[\"labels\"]\n",
    "    oov = []\n",
    "    for i, text in (enumerate(texts)):\n",
    "        seg_texts = []\n",
    "        label = labels[i]\n",
    "        sentence_seg = text.split()\n",
    "        \n",
    "        emb = []\n",
    "\n",
    "        for seg_t in sentence_seg:\n",
    "            if seg_t in W2V.vocab:\n",
    "                emb += [W2V[seg_t]]\n",
    "            else:\n",
    "                oov += [seg_t]\n",
    "\n",
    "        data_dict = {\"emb\": emb, \"label\":label, \n",
    "                     \"src_texts\": text, \"src_label\": src_labels[label],\n",
    "                     \"seg_texts\": seg_texts}\n",
    "        data += [data_dict]\n",
    "    \n",
    "    print(f\"oov: {len(oov)} {len(set(oov))}, {sorted(set(oov))}\")\n",
    "    \n",
    "    return data, num_labels # List[Dict[List]] = List[tokenizer output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov: 331 31, ['3357', '3724', '497766', \"american's\", 'ap57', 'ap68', 'ap80', \"atlanta's\", 'be1', 'd9s', \"delta's\", 'dh8', \"don't\", \"friday's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"it's\", 'j31', 'l1011', \"let's\", 'nationair', \"one's\", \"sunday's\", \"that's\", \"we're\", \"what're\", \"what's\", \"york's\", 'yyz']\n",
      "oov: 55 10, ['137338', 'ap58', 'ap80', 'be1', 'd9s', \"doesn't\", \"i'd\", \"i'm\", 'nationair', \"what's\"]\n"
     ]
    }
   ],
   "source": [
    "data_train, num_labels = get_model_data(f\"data/{DATASET}/train.tsv\")\n",
    "data_test, _ = get_model_data(f\"data/{DATASET}/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_train, f\"bert_data/{DATASET}/transCapsule_train.pt\")\n",
    "torch.save(data_test, f\"bert_data/{DATASET}/transCapsule_test.pt\")\n",
    "\n",
    "del ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 41 if DATASET == \"base\" else 31\n",
    "data_train = torch.load(f\"bert_data/{DATASET}/transCapsule_train.pt\")\n",
    "data_test = torch.load(f\"bert_data/{DATASET}/transCapsule_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 1 11.23373888628919\n",
      "86 i want to travel from kansas city to chicago round trip leaving wednesday june sixteenth arriving in chicago at around 7 o'clock in the evening and returning the next day arriving in kansas city at around 7 o'clock in the evening which airlines fly that route\n",
      "18 what's restriction ap68\n"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "for item in data_train:\n",
    "    ls += [np.array(item[\"emb\"]).shape[0]]\n",
    "ls = np.array(ls)\n",
    "print(ls.max(), ls.min(), ls.mean())\n",
    "print(ls.argmax(), data_train[ls.argmax()][\"src_texts\"])\n",
    "print(ls.argmin(), data_train[ls.argmin()][\"src_texts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_Dataset(Dataset):\n",
    "    def __init__(self, mode, list_of_bert):\n",
    "        assert mode in [\"train\", \"test\", \"dev\"]\n",
    "        self.mode = mode\n",
    "        self.data = list_of_bert\n",
    "    def __getitem__(self, idx):\n",
    "        emb = torch.tensor(self.data[idx][\"emb\"])\n",
    "        label = torch.tensor(self.data[idx][\"label\"])\n",
    "        return emb, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(sample):\n",
    "    sample.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "    embs, labels = zip(*sample)\n",
    "    max_length = MAX_LENGTH\n",
    "    masks = []\n",
    "    pad_embs = []\n",
    "    \n",
    "    \n",
    "    for e in embs:\n",
    "        # padding\n",
    "        pad_len = max_length - e.shape[0]\n",
    "        padding = torch.zeros(pad_len, HIDDEN_SIZE)\n",
    "        pad_embs += [torch.cat((e, padding)).tolist()]\n",
    "        \n",
    "        # attn masling\n",
    "        masking = [1] * e.shape[0] + [0] * pad_len\n",
    "        masks += [masking]\n",
    "        \n",
    "    pad_embs = torch.tensor(pad_embs)\n",
    "    masks = torch.tensor(masks)\n",
    "    labels = torch.tensor(labels)\n",
    "    return pad_embs, masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source from: https://github.com/leftthomas/CapsNet/blob/master/capsule.py\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels, kernel_size=None, stride=None,\n",
    "                 num_iterations=NUM_ROUTING_ITERATIONS):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "\n",
    "        self.num_route_nodes = num_route_nodes\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        if num_route_nodes != -1: # digit_capsules\n",
    "            self.route_weights = nn.Parameter(torch.randn(num_capsules, num_route_nodes, in_channels, out_channels))\n",
    "        else: # primary_capsules\n",
    "            self.capsules = nn.ModuleList(\n",
    "                [nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0) for _ in\n",
    "                 range(num_capsules)])\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_route_nodes != -1: # digit_capsules\n",
    "            priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :] # random initial hat u_j|i\n",
    "            logits = Variable(torch.zeros(*priors.size())) # b_ij = 0\n",
    "            if torch.cuda.is_available():\n",
    "                logits = logits.cuda()\n",
    "            for i in range(self.num_iterations):\n",
    "                probs = F.softmax(logits, dim=2) # c_ij = softmax(b_ij)\n",
    "                outputs = self.squash((probs * priors).sum(dim=2, keepdim=True)) # squash(sum c_ij * hat u_j|i)\n",
    "\n",
    "                if i != self.num_iterations - 1:\n",
    "                    delta_logits = (priors * outputs).sum(dim=-1, keepdim=True) # hat u_j|i * v_j\n",
    "                    logits = logits + delta_logits #b_ij = b_ij + hat u_j|i * v_j\n",
    "        else: # primary_capsules\n",
    "            outputs = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "            outputs = torch.cat(outputs, dim=-1)\n",
    "            outputs = self.squash(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_classifier(nn.Module):\n",
    "    def __init__(self, kernel_size, num_labels, stride=1, hidden_size=HIDDEN_SIZE):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_json_file(\"./config.json\")\n",
    "        self.transformer = BertModel(config, add_pooling_layer=False)\n",
    "#         self.primary_capsules = CapsuleLayer(num_capsules=100, num_route_nodes=-1,\n",
    "#                                 in_channels=hidden_size, out_channels=1,\n",
    "#                                 kernel_size=kernel_size, stride=stride) # output (batch, out_channels * feature_map_size, num_capsules)\n",
    "#         N = int((MAX_LENGTH - kernel_size + 1) / stride)\n",
    "#         self.intent_capsules = CapsuleLayer(num_capsules=num_labels, num_route_nodes=1 * N,\n",
    "#                                             in_channels=100, out_channels=15)\n",
    "\n",
    "        # w/o primary\n",
    "        self.intent_capsules = CapsuleLayer(num_capsules=num_labels, num_route_nodes=300,\n",
    "                                            in_channels=MAX_LENGTH, out_channels=15)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(15 * num_labels, num_labels),\n",
    "                        nn.Softmax(dim=1))\n",
    "    \n",
    "    def forward(self, word_emb, mask):\n",
    "        outputs = self.transformer(inputs_embeds=word_emb, \\\n",
    "                                   attention_mask=mask, return_dict=True)\n",
    "        word_emb = outputs.last_hidden_state \n",
    "        # transpose for conv1d\n",
    "        # (batch, len, hidden_size) -> (batch, channel, len)\n",
    "        word_emb = word_emb.transpose(1,2)\n",
    "#         print(word_emb.shape)\n",
    "#         word_emb = self.primary_capsules(word_emb) # output (batch, out_channels * feature_map_size, num_capsules)\n",
    "#         print(word_emb.shape)\n",
    "        word_emb = self.intent_capsules(word_emb).squeeze().transpose(0, 1)\n",
    "        word_emb = word_emb.flatten(start_dim=1)\n",
    "        intent_class = self.decoder(word_emb)\n",
    "\n",
    "        # origin capsule network\n",
    "#         intent_class = (word_emb ** 2).sum(dim=-1) ** 0.5\n",
    "#         intent_class = F.softmax(intent_class, dim=-1)\n",
    "        \n",
    "        return intent_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "\n",
    "    def forward(self, classes, labels):\n",
    "#         classes = torch.argmax(classes, dim=1)\n",
    "        left = F.relu(0.9 - classes, inplace=True) ** 2\n",
    "        right = F.relu(classes - 0.1, inplace=True) ** 2\n",
    "\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "#         margin_loss.requires_grad = True\n",
    "\n",
    "        return margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: 11802152\n",
      "\n",
      "    name            module\n",
      "    ----------------------\n",
      "transformer     BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 300, padding_idx=0)\n",
      "    (position_embeddings): Embedding(64, 300)\n",
      "    (token_type_embeddings): Embedding(2, 300)\n",
      "    (LayerNorm): LayerNorm((300,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=300, out_features=300, bias=True)\n",
      "            (key): Linear(in_features=300, out_features=300, bias=True)\n",
      "            (value): Linear(in_features=300, out_features=300, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=300, out_features=300, bias=True)\n",
      "            (LayerNorm): LayerNorm((300,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=300, out_features=300, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=300, out_features=300, bias=True)\n",
      "          (LayerNorm): LayerNorm((300,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "intent_capsules CapsuleLayer()\n",
      "decoder         Sequential(\n",
      "  (0): Linear(in_features=255, out_features=17, bias=True)\n",
      "  (1): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = intent_classifier(KERNEL_SIZE, num_labels)\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# loss_func = CapsuleLoss()\n",
    "model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            word_embs, masks, labels = [t.to(device) for t in data if torch.is_tensor(t)]\n",
    "            \n",
    "            intent_cls = model(word_embs, masks)\n",
    "            \n",
    "            _, pred = torch.max(intent_cls, 1) # _: logits最大數值; pred: 最大數值的 index\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "\n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = intent_Dataset(\"train\", data_train)\n",
    "trainLoader = DataLoader(trainSet, batch_size=TRAIN_BATCH_SIZE, collate_fn=minibatch, shuffle=True)\n",
    "testSet = intent_Dataset(\"test\", data_test)\n",
    "testLoader = DataLoader(testSet, batch_size=TEST_BATCH_SIZE, collate_fn=minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorboard logger'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tensorboard logger\"\"\"\n",
    "# writer = SummaryWriter(f\"runs/{DATASET}/transCapsule/B_{TRAIN_BATCH_SIZE}_K_{KERNEL_SIZE}_E_{EPOCHS}_LR_{LR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/535 [00:00<00:15, 34.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:35:00\tstart training model/transCapsule from epoch 1 to 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [00:13<00:00, 38.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:35:14\t[epoch 1] loss: 1492.763\n",
      "[epoch 1] training acc: 0.741226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/535 [00:00<00:14, 37.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] testing acc: 0.723549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [00:14<00:00, 37.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:35:39\t[epoch 2] loss: 1372.442\n",
      "[epoch 2] training acc: 0.742162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/535 [00:00<00:15, 35.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] testing acc: 0.723549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [00:14<00:00, 37.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:36:04\t[epoch 3] loss: 1239.590\n",
      "[epoch 3] training acc: 0.742396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/535 [00:00<00:14, 36.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] testing acc: 0.723549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [00:14<00:00, 37.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:36:29\t[epoch 4] loss: 1199.519\n",
      "[epoch 4] training acc: 0.742396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/535 [00:00<00:13, 38.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] testing acc: 0.723549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [00:13<00:00, 38.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:36:53\t[epoch 5] loss: 1186.553\n",
      "[epoch 5] training acc: 0.742396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/535 [00:00<00:13, 38.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] testing acc: 0.723549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [00:13<00:00, 39.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 15:37:17\t[epoch 6] loss: 1180.723\n",
      "[epoch 6] training acc: 0.742396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/535 [00:00<00:14, 36.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] testing acc: 0.723549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 135/535 [00:03<00:10, 38.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d36ce5c4fc35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mword_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-a4ad55107381>\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0membs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a4ad55107381>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0membs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_from = 0\n",
    "# EPOCHS = 20\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "timestamp(f\"start training {MODEL_PATH} from epoch {train_from+1} to {EPOCHS}\")\n",
    "for epoch in range(train_from, EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(trainLoader):\n",
    "        word_embs, masks, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(word_embs, masks)\n",
    "#         break\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        # origin margin loss\n",
    "#         labels_onehot = torch.FloatTensor(labels.shape[0], num_labels).to(device)\n",
    "#         labels_onehot.zero_()\n",
    "#         labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "#         loss = loss_func(outputs, labels_onehot)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "#     break\n",
    "\n",
    "    timestamp(f\"[epoch {epoch+1}] loss: {running_loss:.3f}\")\n",
    "#     writer.add_scalar('Loss/cls', running_loss, epoch)\n",
    "\n",
    "    _, acc = get_predictions(model, trainLoader, compute_acc=True)\n",
    "    print(f\"[epoch {epoch+1}] training acc: {acc:.6f}\")\n",
    "#     writer.add_scalar('Acc/train', acc, epoch)\n",
    "\n",
    "    _, acc = get_predictions(model, testLoader, compute_acc=True)\n",
    "    print(f\"[epoch {epoch+1}] testing acc: {acc:.6f}\")\n",
    "#     writer.add_scalar('Acc/test', acc, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "scope/x:0 scope/x:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.variable_scope('scope'):\n",
    "    v1 = tf.get_variable('x', [1])\n",
    "#     v1 = tf.Variable(0, name=\"x\")\n",
    "with tf.variable_scope('scope', reuse=True):\n",
    "    v2 = tf.get_variable('x', [1])\n",
    "#     v2 = tf.Variable(1, name=\"x\")\n",
    "\n",
    "print(v1.name, v2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "class a():\n",
    "    def __init__(self, in_):\n",
    "        self.a = in_\n",
    "        \n",
    "    def __call__(self, in_):\n",
    "        self.a += in_\n",
    "        return self.a\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.a)\n",
    "        \n",
    "print(a(5)(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

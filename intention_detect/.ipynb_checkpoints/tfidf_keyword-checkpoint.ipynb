{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"IVR\"\n",
    "# WS_SET = [\"jieba\", \"ckip\", \"mix_0\", \"mix_1\"]\n",
    "# WORD_SEGMENTER = WS_SET[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_tfidf(corpus, voc, idf):\n",
    "    count_vectorizer = CountVectorizer(vocabulary=voc, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    tf = count_vectorizer.fit_transform(corpus)\n",
    "    tfidf = (np.ma.log(tf.toarray()) + 1) * idf\n",
    "    tfidf = tfidf.filled(0)\n",
    "    \n",
    "    return normalize(tfidf, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(test_tfidf, train_tfidf, num_labels=None):\n",
    "    if num_labels is None:\n",
    "        sim_array = cosine_similarity(test_tfidf, train_tfidf)\n",
    "    else:\n",
    "        train_tfidf_by_class = []\n",
    "        start_idx = 0\n",
    "        for i in num_labels:\n",
    "            train_tfidf_by_class += [train_tfidf[start_idx : start_idx+i].mean(axis=0)]\n",
    "            start_idx += i\n",
    "        train_tfidf_by_class = np.stack(train_tfidf_by_class)\n",
    "        sim_array = cosine_similarity(test_tfidf, train_tfidf_by_class)\n",
    "    \n",
    "    return np.array(sim_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sim_array, test_data, train_data, sim_mode):\n",
    "    correct = 0\n",
    "    for i, ans in enumerate(test_data[\"label\"]):\n",
    "        if sim_mode == 0:\n",
    "            sorted_arg = np.argsort(sim_array[i])\n",
    "            sorted_arg = np.flip(sorted_arg)[:10] # 10 for top 10 similar\n",
    "            sorted_pred = [train_data[\"label\"][idx] for idx in sorted_arg]\n",
    "            pred = max(set(sorted_pred), key=sorted_pred.count)\n",
    "        else:\n",
    "            pred = np.argmax(sim_array[i])\n",
    "    \n",
    "        if pred == ans:\n",
    "            correct += 1\n",
    "    acc = correct / len(test_data[\"label\"])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df = df.sort_values(by=[\"labels\"], ignore_index=True)\n",
    "    src_labels = sorted(set(df.labels.tolist()))\n",
    "    df[\"labels\"] = [src_labels.index(l) for l in df.labels.tolist()]\n",
    "    texts = df[\"texts\"]\n",
    "    labels = df[\"labels\"]\n",
    "    num_labels = list(Counter(labels).values())\n",
    "    \n",
    "    data = {\"corpus\":[], \"label\":[], \"src_texts\":[], \"src_label\":[]}\n",
    "    \n",
    "    for i, t in (enumerate(texts)):\n",
    "        label = labels[i]\n",
    "\n",
    "#         if WORD_SEGMENTER == \"ckip\":\n",
    "#             sentence_seg = ws([t])[0]\n",
    "#         elif WORD_SEGMENTER == \"jieba\":\n",
    "#             sentence_seg = jieba.lcut(t)\n",
    "#         elif WORD_SEGMENTER == \"mix_0\":\n",
    "#             temp = ws([t])[0]\n",
    "#             sentence_seg = []\n",
    "#             for seg_t in temp:\n",
    "#                 sentence_seg += jieba.lcut(seg_t)\n",
    "#         elif WORD_SEGMENTER == \"mix_1\":\n",
    "#             temp = jieba.lcut(t)\n",
    "#             sentence_seg = []\n",
    "#             for seg_t in temp:\n",
    "#                 sentence_seg += ws([seg_t])[0]\n",
    "    \n",
    "#         sentence_seg = [seg_t for seg_t in sentence_seg if seg_t!=' ']\n",
    "#         seg_texts = ' '.join(sentence_seg)\n",
    "        \n",
    "#         data[\"corpus\"] += [seg_texts]\n",
    "        data[\"label\"] += [label]\n",
    "        data[\"src_texts\"] += [t]\n",
    "        data[\"src_label\"] += [src_labels[label]]\n",
    "    return data, num_labels # Dict[List], List[Int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, num_labels = get_model_data(f\"data/{DATASET}/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test, _ = get_model_data(f\"data/{DATASET}/test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"data/{DATASET}/train.tsv\", sep='\\t')\n",
    "\n",
    "df.tf = df.tf.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2144, 107)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(sublinear_tf=True)\n",
    "# train_tfidf = tfidf_vectorizer.fit_transform(data_train[\"corpus\"])\n",
    "df = pd.read_csv(f\"data/{DATASET}/train.tsv\", sep='\\t')\n",
    "df.tf = df.tf.apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "train_tf = np.stack(df.tf.tolist())\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_tf)\n",
    "idf = tfidf_transformer.idf_\n",
    "# voc_list = tfidf_vectorizer.get_feature_names()\n",
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548, 107)\n"
     ]
    }
   ],
   "source": [
    "# test_tfidf = get_tfidf(data_test[\"corpus\"], voc_list, idf)\n",
    "df = pd.read_csv(f\"data/{DATASET}/test.tsv\", sep='\\t')\n",
    "df.tf = df.tf.apply(ast.literal_eval)\n",
    "\n",
    "test_tf = np.stack(df.tf.tolist())\n",
    "test_tfidf = (np.ma.log(test_tf) + 1) * idf\n",
    "test_tfidf = test_tfidf.filled(0)\n",
    "\n",
    "test_tfidf = normalize(test_tfidf, norm='l2')\n",
    "print(test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548, 2144)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45072992700729925"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SIM_MODE = 0\n",
    "sim_array = get_similarity(test_tfidf, train_tfidf)\n",
    "print(sim_array.shape)\n",
    "get_prediction(sim_array, data_test, data_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548, 63)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4762773722627737"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SIM_MODE = 1\n",
    "sim_array = get_similarity(test_tfidf, train_tfidf, num_labels)\n",
    "print(sim_array.shape)\n",
    "get_prediction(sim_array, data_test, data_train, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = [\"卡\", \"卡片\"]\n",
    "corpus = [\"我的卡片多少錢\", \"account activate\"]\n",
    "count_vectorizer = CountVectorizer(vocabulary=voc, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "tf = count_vectorizer.fit_transform(corpus)\n",
    "tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

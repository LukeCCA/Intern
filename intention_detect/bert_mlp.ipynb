{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer_LM = \"../bert-base-chinese\"\n",
    "NO = 1\n",
    "LM_SET = [\"../bert-base-chinese\", \"../ckiplab/bert-base-chinese\", \"../hfl/chinese-bert-wwm\", \"../hfl/rbtl3\"]\n",
    "MODEL_NAME_SET = [\"base\", \"ckip\", \"wwm\", \"rbtl3\"]\n",
    "LM = LM_SET[NO]\n",
    "MODEL_NAME = MODEL_NAME_SET[NO] + '_mlp'\n",
    "DATASET = \"SMP2018\"\n",
    "BATCH_SIZE = 16\n",
    "MODEL_PATH = f\"model/{DATASET}/{MODEL_NAME}\" # svae/load model name/path\n",
    "EPOCHS = 50\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timezone,timedelta\n",
    "def timestamp(msg=\"\"):\n",
    "    dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "    print(str(dt2)[:-13] + '\\t' + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-level 顯示此模型裡的 modules\n",
    "def model_info(model):\n",
    "#     print(model.device)\n",
    "    print(\"\"\"\n",
    "    name            module\n",
    "    ----------------------\"\"\")\n",
    "    for name, module in model.named_children():\n",
    "        if name == \"bert\" or name==\"0\":\n",
    "            for n, _ in module.named_children():\n",
    "                print(f\"{name}:{n}\")\n",
    "    #             print(_)\n",
    "        else:\n",
    "            print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_data(mode, file_path):\n",
    "    assert mode in [\"train\", \"test\", \"dev\"]\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    src_labels = sorted(set(df.labels.tolist()))\n",
    "    num_labels = len(src_labels)\n",
    "    df[\"labels\"] = [src_labels.index(l) for l in df.labels.tolist()]\n",
    "    bert_data = []\n",
    "    texts = df[\"texts\"]\n",
    "\n",
    "    labels = df[\"labels\"]\n",
    "    for i, t in enumerate(texts):\n",
    "        label = labels[i]\n",
    "        bert_dict = {\"label\": label, \"src_texts\": t, \"src_label\": src_labels[label]}\n",
    "        bert_dict.update(\n",
    "            tokenizer(t, \n",
    "                      max_length=128,\n",
    "                      padding='max_length',\n",
    "                      return_token_type_ids=True,\n",
    "                      truncation=True,\n",
    "                      ))\n",
    "        bert_data += [bert_dict]\n",
    "    torch.save(bert_data, f\"bert_data/{mode}.pt\")\n",
    "    if mode == \"train\":\n",
    "        return bert_data, num_labels #List[Dict[List]] = List[tokenizer output]\n",
    "    else:\n",
    "        return bert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2299 31\n"
     ]
    }
   ],
   "source": [
    "\"\"\"training data\"\"\"\n",
    "bert_train, num_labels = get_bert_data(\"train\", f\"data/{DATASET}/train.tsv\")\n",
    "print(len(bert_train), num_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"devlopment data\"\"\"\n",
    "bert_dev = get_bert_data(\"dev\", f\"data/{DATASET}/valid.tsv\")\n",
    "len(bert_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"testing data\"\"\"\n",
    "bert_test = get_bert_data(\"test\", f\"data/{DATASET}/test.tsv\")\n",
    "len(bert_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class intent_Dataset(Dataset):\n",
    "    def __init__(self, mode, list_of_bert):\n",
    "        assert mode in [\"train\", \"test\", \"dev\"]\n",
    "        self.mode = mode\n",
    "        self.data = list_of_bert\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data[idx][\"input_ids\"])\n",
    "        seg_emb = torch.tensor(self.data[idx][\"token_type_ids\"])\n",
    "        att_emb = torch.tensor(self.data[idx][\"attention_mask\"])\n",
    "#         if self.mode == \"train\" or self.mode == \"dev\":\n",
    "        label = torch.tensor(self.data[idx][\"label\"])\n",
    "        return input_ids, seg_emb, att_emb, label\n",
    "#         else:\n",
    "#             return input_ids, seg_emb, att_emb\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_classifier(nn.Module):\n",
    "    def __init__(self, LM, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(LM, return_dict=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(512, num_labels),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, \n",
    "                input_ids=None,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=None):\n",
    "        bert_outputs = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        cls_token = bert_outputs.last_hidden_state[:, 0, :]\n",
    "        output = self.mlp(cls_token)\n",
    "        return output\n",
    "\n",
    "model = intent_classifier(LM, num_labels)\n",
    "optimizer = torch.optim.SGD(model.mlp.parameters(), lr=0.7) # follow paper fix LM parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    name            module\n",
      "    ----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "mlp             Sequential(\n",
      "  (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.75, inplace=False)\n",
      "  (3): Linear(in_features=512, out_features=31, bias=True)\n",
      "  (4): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = intent_Dataset(\"train\", bert_train)\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# devSet = intent_Dataset(\"dev\", bert_dev)\n",
    "# devLoader = DataLoader(devSet, batch_size=BATCH_SIZE*2)\n",
    "testSet = intent_Dataset(\"test\", bert_test)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in (dataloader):\n",
    "            tokens_tensors, segments_tensors, masks_tensors,\\\n",
    "            labels = [t.to(device) for t in data if t is not None]\n",
    "            \n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            _, pred = torch.max(outputs.data, 1) # _: logits最大數值; pred: 最大數值的 index\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"tensorboard logger\"\"\"\n",
    "writer = SummaryWriter(f\"runs/{DATASET}/{MODEL_NAME}/E_{EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-01 10:56:09\tstart training model/SMP2018/ckip_mlp from epoch 1 to 50\n",
      "2021-04-01 10:56:32\t[epoch 1] loss: 474.320\n",
      "[epoch 1] training acc: 0.197912\n",
      "[epoch 1] testing acc: 0.200000\n",
      "2021-04-01 10:57:07\t[epoch 2] loss: 473.811\n",
      "[epoch 2] training acc: 0.197912\n",
      "[epoch 2] testing acc: 0.200000\n",
      "2021-04-01 10:57:42\t[epoch 3] loss: 473.775\n",
      "[epoch 3] training acc: 0.197912\n",
      "[epoch 3] testing acc: 0.200000\n",
      "2021-04-01 10:58:17\t[epoch 4] loss: 473.768\n",
      "[epoch 4] training acc: 0.197912\n",
      "[epoch 4] testing acc: 0.200000\n",
      "2021-04-01 10:58:52\t[epoch 5] loss: 473.741\n",
      "[epoch 5] training acc: 0.197912\n",
      "[epoch 5] testing acc: 0.200000\n",
      "2021-04-01 10:59:27\t[epoch 6] loss: 473.776\n",
      "[epoch 6] training acc: 0.197912\n",
      "[epoch 6] testing acc: 0.200000\n",
      "2021-04-01 11:00:03\t[epoch 7] loss: 473.768\n",
      "[epoch 7] training acc: 0.197912\n",
      "[epoch 7] testing acc: 0.200000\n",
      "2021-04-01 11:00:38\t[epoch 8] loss: 473.799\n",
      "[epoch 8] training acc: 0.197912\n",
      "[epoch 8] testing acc: 0.200000\n",
      "2021-04-01 11:01:14\t[epoch 9] loss: 473.795\n",
      "[epoch 9] training acc: 0.197912\n",
      "[epoch 9] testing acc: 0.200000\n",
      "2021-04-01 11:01:49\t[epoch 10] loss: 473.797\n",
      "[epoch 10] training acc: 0.197912\n",
      "[epoch 10] testing acc: 0.200000\n",
      "2021-04-01 11:02:25\t[epoch 11] loss: 473.740\n",
      "[epoch 11] training acc: 0.197912\n",
      "[epoch 11] testing acc: 0.200000\n",
      "2021-04-01 11:03:00\t[epoch 12] loss: 473.797\n",
      "[epoch 12] training acc: 0.197912\n",
      "[epoch 12] testing acc: 0.200000\n",
      "2021-04-01 11:03:36\t[epoch 13] loss: 473.768\n",
      "[epoch 13] training acc: 0.197912\n",
      "[epoch 13] testing acc: 0.200000\n",
      "2021-04-01 11:04:11\t[epoch 14] loss: 473.825\n",
      "[epoch 14] training acc: 0.197912\n",
      "[epoch 14] testing acc: 0.200000\n",
      "2021-04-01 11:04:47\t[epoch 15] loss: 473.825\n",
      "[epoch 15] training acc: 0.197912\n",
      "[epoch 15] testing acc: 0.200000\n",
      "2021-04-01 11:05:23\t[epoch 16] loss: 473.797\n",
      "[epoch 16] training acc: 0.197912\n",
      "[epoch 16] testing acc: 0.200000\n",
      "2021-04-01 11:05:58\t[epoch 17] loss: 473.740\n",
      "[epoch 17] training acc: 0.197912\n",
      "[epoch 17] testing acc: 0.200000\n",
      "2021-04-01 11:06:34\t[epoch 18] loss: 473.796\n",
      "[epoch 18] training acc: 0.197912\n",
      "[epoch 18] testing acc: 0.200000\n",
      "2021-04-01 11:07:09\t[epoch 19] loss: 473.769\n",
      "[epoch 19] training acc: 0.197912\n",
      "[epoch 19] testing acc: 0.200000\n",
      "2021-04-01 11:07:45\t[epoch 20] loss: 473.711\n",
      "[epoch 20] training acc: 0.197912\n",
      "[epoch 20] testing acc: 0.200000\n",
      "2021-04-01 11:08:21\t[epoch 21] loss: 473.740\n",
      "[epoch 21] training acc: 0.197912\n",
      "[epoch 21] testing acc: 0.200000\n",
      "2021-04-01 11:08:56\t[epoch 22] loss: 473.769\n",
      "[epoch 22] training acc: 0.197912\n",
      "[epoch 22] testing acc: 0.200000\n",
      "2021-04-01 11:09:32\t[epoch 23] loss: 473.796\n",
      "[epoch 23] training acc: 0.197912\n",
      "[epoch 23] testing acc: 0.200000\n",
      "2021-04-01 11:10:08\t[epoch 24] loss: 473.825\n",
      "[epoch 24] training acc: 0.197912\n",
      "[epoch 24] testing acc: 0.200000\n",
      "2021-04-01 11:10:43\t[epoch 25] loss: 473.768\n",
      "[epoch 25] training acc: 0.197912\n",
      "[epoch 25] testing acc: 0.200000\n",
      "2021-04-01 11:11:19\t[epoch 26] loss: 473.826\n",
      "[epoch 26] training acc: 0.197912\n",
      "[epoch 26] testing acc: 0.200000\n",
      "2021-04-01 11:11:54\t[epoch 27] loss: 473.740\n",
      "[epoch 27] training acc: 0.197912\n",
      "[epoch 27] testing acc: 0.200000\n",
      "2021-04-01 11:12:30\t[epoch 28] loss: 473.768\n",
      "[epoch 28] training acc: 0.197912\n",
      "[epoch 28] testing acc: 0.200000\n",
      "2021-04-01 11:13:06\t[epoch 29] loss: 473.740\n",
      "[epoch 29] training acc: 0.197912\n",
      "[epoch 29] testing acc: 0.200000\n",
      "2021-04-01 11:13:41\t[epoch 30] loss: 473.719\n",
      "[epoch 30] training acc: 0.197912\n",
      "[epoch 30] testing acc: 0.200000\n",
      "2021-04-01 11:14:17\t[epoch 31] loss: 473.768\n",
      "[epoch 31] training acc: 0.197912\n",
      "[epoch 31] testing acc: 0.200000\n",
      "2021-04-01 11:14:52\t[epoch 32] loss: 473.683\n",
      "[epoch 32] training acc: 0.197912\n",
      "[epoch 32] testing acc: 0.200000\n",
      "2021-04-01 11:15:28\t[epoch 33] loss: 473.825\n",
      "[epoch 33] training acc: 0.197912\n",
      "[epoch 33] testing acc: 0.200000\n",
      "2021-04-01 11:16:04\t[epoch 34] loss: 473.740\n",
      "[epoch 34] training acc: 0.197912\n",
      "[epoch 34] testing acc: 0.200000\n",
      "2021-04-01 11:16:39\t[epoch 35] loss: 473.740\n",
      "[epoch 35] training acc: 0.197912\n",
      "[epoch 35] testing acc: 0.200000\n",
      "2021-04-01 11:17:15\t[epoch 36] loss: 473.740\n",
      "[epoch 36] training acc: 0.197912\n",
      "[epoch 36] testing acc: 0.200000\n",
      "2021-04-01 11:17:50\t[epoch 37] loss: 473.740\n",
      "[epoch 37] training acc: 0.197912\n",
      "[epoch 37] testing acc: 0.200000\n",
      "2021-04-01 11:18:26\t[epoch 38] loss: 473.711\n",
      "[epoch 38] training acc: 0.197912\n",
      "[epoch 38] testing acc: 0.200000\n",
      "2021-04-01 11:19:02\t[epoch 39] loss: 473.711\n",
      "[epoch 39] training acc: 0.197912\n",
      "[epoch 39] testing acc: 0.200000\n",
      "2021-04-01 11:19:37\t[epoch 40] loss: 473.711\n",
      "[epoch 40] training acc: 0.197912\n",
      "[epoch 40] testing acc: 0.200000\n",
      "2021-04-01 11:20:13\t[epoch 41] loss: 473.740\n",
      "[epoch 41] training acc: 0.197912\n",
      "[epoch 41] testing acc: 0.200000\n",
      "2021-04-01 11:20:49\t[epoch 42] loss: 473.797\n",
      "[epoch 42] training acc: 0.197912\n",
      "[epoch 42] testing acc: 0.200000\n",
      "2021-04-01 11:21:24\t[epoch 43] loss: 473.826\n",
      "[epoch 43] training acc: 0.197912\n",
      "[epoch 43] testing acc: 0.200000\n",
      "2021-04-01 11:22:00\t[epoch 44] loss: 473.740\n",
      "[epoch 44] training acc: 0.197912\n",
      "[epoch 44] testing acc: 0.200000\n",
      "2021-04-01 11:22:36\t[epoch 45] loss: 473.797\n",
      "[epoch 45] training acc: 0.197912\n",
      "[epoch 45] testing acc: 0.200000\n",
      "2021-04-01 11:23:11\t[epoch 46] loss: 473.797\n",
      "[epoch 46] training acc: 0.197912\n",
      "[epoch 46] testing acc: 0.200000\n",
      "2021-04-01 11:23:47\t[epoch 47] loss: 473.740\n",
      "[epoch 47] training acc: 0.197912\n",
      "[epoch 47] testing acc: 0.200000\n",
      "2021-04-01 11:24:23\t[epoch 48] loss: 473.712\n",
      "[epoch 48] training acc: 0.197912\n",
      "[epoch 48] testing acc: 0.200000\n",
      "2021-04-01 11:24:58\t[epoch 49] loss: 473.797\n",
      "[epoch 49] training acc: 0.197912\n",
      "[epoch 49] testing acc: 0.200000\n",
      "2021-04-01 11:25:34\t[epoch 50] loss: 473.741\n",
      "[epoch 50] training acc: 0.197912\n",
      "[epoch 50] testing acc: 0.200000\n"
     ]
    }
   ],
   "source": [
    "train_from = 0\n",
    "if MODEL_PATH.find(\".pt\") != -1:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    p = MODEL_PATH.rfind('_')\n",
    "    train_from = int(MODEL_PATH[p+1 : -3])\n",
    "    MODEL_PATH = MODEL_PATH[: p-2]\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "timestamp(f\"start training {MODEL_PATH} from epoch {train_from+1} to {EPOCHS}\")\n",
    "for epoch in range(train_from, EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in (trainLoader):\n",
    "        tokens_tensors, segments_tensors, masks_tensors, \\\n",
    "        labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids = tokens_tensors, \n",
    "                        token_type_ids = segments_tensors, \n",
    "                        attention_mask = masks_tensors)\n",
    "        \n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss = loss_func(outputs, labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "#     torch.save(model.state_dict(), F\"{MODEL_PATH}_E_{str(epoch+1)}.pt\")\n",
    "    timestamp(f\"[epoch {epoch+1}] loss: {running_loss:.3f}\")\n",
    "    writer.add_scalar('Loss/cls', running_loss, epoch)\n",
    "    \n",
    "    _, acc = get_predictions(model, trainLoader, compute_acc=True)\n",
    "    print(f\"[epoch {epoch+1}] training acc: {acc:.6f}\")\n",
    "    writer.add_scalar('Acc/train', acc, epoch)\n",
    "\n",
    "#     _, acc = get_predictions(model, devLoader, compute_acc=True)\n",
    "#     print(f\"[epoch {epoch+1}] validation acc: {acc:.6f}\")\n",
    "    _, acc = get_predictions(model, testLoader, compute_acc=True)\n",
    "    print(f\"[epoch {epoch+1}] testing acc: {acc:.6f}\")\n",
    "    writer.add_scalar('Acc/test', acc, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet = intent_Dataset(\"test\", bert_test)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1,2):\n",
    "    model.load_state_dict(torch.load(f\"{MODEL_PATH}_E_{e}.pt\"))\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    _, acc = get_predictions(model, testLoader, compute_acc=True)\n",
    "    print(f\"[epoch {e}] testing acc: {acc:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 7])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(2, 5, 7)\n",
    "# With Learnable Parameters\n",
    "m = nn.LayerNorm(input.size()[1:])\n",
    "# Without Learnable Parameters\n",
    "# m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n",
    "# Normalize over last two dimensions\n",
    "# m = nn.LayerNorm([10, 10])\n",
    "# Normalize over last dimension of size 10\n",
    "# m = nn.LayerNorm(7)\n",
    "# Activating the module\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 7])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
